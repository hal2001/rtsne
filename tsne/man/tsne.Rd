% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tsne.R
\name{tsne}
\alias{tsne}
\title{Embed a dataset using t-distributed stochastic neighbor embedding.}
\usage{
tsne(X, k = 2, scale = "range", init = "rand", perplexity = 30,
  inp_kernel = "gauss", max_iter = 1000, whiten = FALSE,
  whiten_dims = 30, epoch_callback = NULL, epoch = 100, min_cost = 0,
  momentum = 0.5, final_momentum = 0.8, mom_switch_iter = 250,
  eta = 500, min_gain = 0.01, exaggerate = 4,
  exaggeration_off_iter = 100)
}
\arguments{
\item{X}{Input coordinates or distance matrix.}

\item{k}{Number of output dimensions for the embedding.}

\item{scale}{How to preprocess \code{X}. One of: \code{"none"} or
(\code{NULL}), which applies no further preprocessing; \code{"range"},
which range scales the matrix elements between 0 and 1; \code{"bh"}, which
applies the same scaling in Barnes-Hut t-SNE, where the columns are mean
centered and then the elements divided by absolute maximum value.}

\item{init}{How to initialize the output coordinates. One of: \code{"rand"},
which initializes from a Gaussian distribution with mean 0 and standard
deviation 1e-4; \code{"pca"}, which uses the first \code{k} scores of the
PCA: columns are centered, but no scaling beyond that which is applied by
the \code{scale} parameter is carried out; \code{"spca"}, which uses the
PCA scores and then scales each score to a standard deviation of 1e-4; or a
matrix can be used to set the coordinates directly. It must have dimensions
\code{n} by \code{k}, where \code{n} is the number of rows in \code{X}.}

\item{perplexity}{The target perplexity for parameterizing the input
probabilities.}

\item{inp_kernel}{The input kernel function. Can be either \code{"gauss"}
(the default), or \code{"exp"}, which uses the unsquared distances.
\code{"exp"} is not the usual literature function, but matches the original
rtsne implementation (and it probably doesn't matter very much).}

\item{max_iter}{Maximum number of iterations in the optimization.}

\item{whiten}{If \code{TRUE}, whitens the input data before calculating the
input probabilities.}

\item{whiten_dims}{Number of dimensions to use if the data is preprocessed by
whitening. Must not be greater than the number of columns in \code{X}.}

\item{epoch_callback}{Function to call after each epoch. Should have the
signature \code{epoch_callback(Y)} where \code{Y} is the output coordinate
matrix.}

\item{epoch}{After every \code{epoch} number of steps, calculates and
displays the cost value and calls \code{epoch_callback}, if supplied.}

\item{min_cost}{If the cost falls below this value, the optimization will
stop early.}

\item{momentum}{Initial momentum value.}

\item{final_momentum}{Final momentum value.}

\item{mom_switch_iter}{Iteration at which the momentum will switch from
\code{momentum} to \code{final_momentum}.}

\item{eta}{Learning rate value.}

\item{min_gain}{Minimum gradient descent step size.}

\item{exaggerate}{Numerical value to multiply input probabilities by, during
the early exaggeration phase. Not used if \code{initial_config} is not
\code{NULL}. May also provide the string \code{"ls"}, in which case the
dataset-dependent exaggeration technique suggested by Linderman and
Steinerberger (2017) is used.}

\item{exaggeration_off_iter}{Iteration at which early exaggeration is turned
off.}
}
\value{
The embedded output coordinates.
}
\description{
Embed a dataset using t-distributed stochastic neighbor embedding.
}
\examples{
\dontrun{
colors = rainbow(length(unique(iris$Species)))
names(colors) = unique(iris$Species)
ecb = function(x, y) {
  plot(x, t = 'n')
  text(x, labels = iris$Species, col = colors[iris$Species])
}
tsne_iris <- tsne(iris[, -5], epoch_callback = ecb, perplexity = 50)
# Use the early exaggeration suggested by Linderman and Steinerberger
tsne_iris_ls <- tsne(iris[, -5], epoch_callback = ecb, perplexity = 50, exaggerate = "ls")
# Make embedding deterministic by initializing with scaled PCA scores
tsne_iris_spca <- tsne(iris[, -5], epoch_callback = ecb, perplexity = 50, exaggerate = "ls",
                       scale = "spca")
}
}
\references{
Van der Maaten, L., & Hinton, G. (2008).
Visualizing data using t-SNE.
\emph{Journal of Machine Learning Research}, \emph{9} (2579-2605).
\url{http://www.jmlr.org/papers/v9/vandermaaten08a.html}

Linderman, G. C., & Steinerberger, S. (2017).
Clustering with t-SNE, provably.
\emph{arXiv preprint} \emph{arXiv}:1706.02582.
\url{https://arxiv.org/abs/1706.02582}
}
