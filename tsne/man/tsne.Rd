% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tsne.R
\name{tsne}
\alias{tsne}
\title{t-Distributed Stochastic Neighbor Embedding}
\usage{
tsne(X, k = 2, scale = "range", Y_init = "rand", perplexity = 30,
  inp_kernel = "gauss", max_iter = 1000, pca = FALSE, initial_dims = 50,
  whiten = FALSE, whiten_dims = 30, epoch_callback = NULL,
  epoch = base::round(max_iter/10), min_cost = 0, momentum = 0.5,
  final_momentum = 0.8, mom_switch_iter = 250, eta = 500,
  min_gain = 0.01, exaggeration_factor = 4, stop_lying_iter = 100,
  ret_extra = FALSE, verbose = FALSE)
}
\arguments{
\item{X}{Input coordinates or distance matrix.}

\item{k}{Number of output dimensions for the embedding.}

\item{scale}{If \code{TRUE}, scale each column to zero mean and unit
variance. Alternatively, you may specify one of the following strings:
\code{"range"}, which range scales the matrix elements between 0 and 1;
\code{"bh"}, which applies the same scaling in Barnes-Hut t-SNE, where the
columns are mean centered and then the elements divided by absolute maximum
value; \code{"scale"} does the same as using \code{TRUE}. To use the input
data as-is, use \code{FALSE}, \code{NULL} or \code{"none"}.}

\item{Y_init}{How to initialize the output coordinates. One of: \code{"rand"},
which initializes from a Gaussian distribution with mean 0 and standard
deviation 1e-4; \code{"pca"}, which uses the first \code{k} scores of the
PCA: columns are centered, but no scaling beyond that which is applied by
the \code{scale} parameter is carried out; \code{"spca"}, which uses the
PCA scores and then scales each score to a standard deviation of 1e-4; or a
matrix can be used to set the coordinates directly. It must have dimensions
\code{n} by \code{k}, where \code{n} is the number of rows in \code{X}.}

\item{perplexity}{The target perplexity for parameterizing the input
probabilities.}

\item{inp_kernel}{The input kernel function. Can be either \code{"gauss"}
(the default), or \code{"exp"}, which uses the unsquared distances.
\code{"exp"} is not the usual literature function, but matches the original
rtsne implementation (and it probably doesn't matter very much).}

\item{max_iter}{Maximum number of iterations in the optimization.}

\item{pca}{If \code{TRUE}, apply PCA to reduce the dimensionality of
\code{X} before any perplexity calibration, but after apply any scaling
and filtering. The number of principal components to keep is specified by
\code{initial_dims}.}

\item{initial_dims, }{if \code{pca = TRUE}, the number of principal components
to keep.}

\item{whiten}{If \code{TRUE}, whitens the input data before calculating the
input probabilities.}

\item{whiten_dims}{Number of dimensions to use if the data is preprocessed by
whitening. Must not be greater than the number of columns in \code{X}.}

\item{epoch_callback}{Function to call after each epoch. Should have the
signature \code{epoch_callback(Y)} where \code{Y} is the output coordinate
matrix.}

\item{epoch}{After every \code{epoch} number of steps, calculates and
displays the cost value and calls \code{epoch_callback}, if supplied.}

\item{min_cost}{If the cost falls below this value, the optimization will
stop early.}

\item{momentum}{Initial momentum value.}

\item{final_momentum}{Final momentum value.}

\item{mom_switch_iter}{Iteration at which the momentum will switch from
\code{momentum} to \code{final_momentum}.}

\item{eta}{Learning rate value.}

\item{min_gain}{Minimum gradient descent step size.}

\item{exaggeration_factor}{Numerical value to multiply input probabilities
by, during the early exaggeration phase. Not used if \code{Y_init} is a
matrix. May also provide the string \code{"ls"}, in which case the
dataset-dependent exaggeration technique suggested by Linderman and
Steinerberger (2017) is used.}

\item{stop_lying_iter}{Iteration at which early exaggeration is turned
off.}

\item{ret_extra}{If \code{TRUE}, return value is a list containing additional
values associated with the t-SNE procedure; otherwise just the output
coordinates. You may also provide a vector of names of potentially large or
expensive-to-calculate values to return, which will be returned in addition
to those value which are returned when this value is \code{TRUE}. See the
\code{Value} section for details.}

\item{verbose}{If \code{TRUE}, log progress messages to the console.}
}
\value{
If \code{ret_extra} is \code{FALSE}, the embedded output coordinates
  as a matrix. Otherwise, a list with the following items:
\itemize{
\item{\code{Y}} Matrix containing the embedded output coordinates.
\item{\code{N}} Number of objects.
\item{\code{origD}} Dimensionality of the input data.
\item{\code{scale}} Scaling applied to input data, as specified by the
  \code{scale} parameter.
\item{\code{Y_init}} Initialization type of the output coordinates, as
  specified by the \code{Y_init} parameter, or if a matrix was used, this will
  contain the string \code{"matrix"}.
\item{\code{iter}} Number of iterations the optimization carried out.
\item{\code{time_secs}} Time taken for the embedding, in seconds.
\item{\code{perplexity}} Target perplexity of the input probabilities, as
  specified by the \code{perplexity} parameter.
\item{\code{costs}} Embedding error associated with each observation. This is
  the sum of the absolute value of each component of the KL cost that the
  observation is associated with, so don't expect these to sum to the
  reported KL cost.
\item{\code{itercosts}} KL cost at each epoch.
\item{\code{stop_lying_iter}} Iteration at which early exaggeration is
  stopped, as specified by the \code{stop_lying_iter} parameter.
\item{\code{mom_switch_iter}} Iteration at which momentum used in
  optimization switches from \code{momentum} to \code{final_momentum}, as
  specified by the \code{mom_switch_iter} parameter.
\item{\code{momentum}} Momentum used in the initial part of the optimization,
  as specified by the \code{momentum} parameter.
\item{\code{final_momentum}} Momentum used in the second part of the
  optimization, as specified by the \code{final_momentum} parameter.
\item{\code{eta}} Learning rate, as specified by the \code{eta} parameter.
\item{\code{exaggeration_factor}} Multiplier of the input probabilities
  during the exaggeration phase. If the Linderman-Steinerberger exaggeration
  scheme is used, this value will have the name \code{"ls"}.
\item{\code{pca_dims}} If PCA was carried out to reduce the initial
  dimensionality of the input, the number of components retained, as
  specified by the \code{initial_dims} parameter.
\item{\code{whiten_dims}} If whitening was carried out to reduce the initial
  dimensionality of the input, the number of components retained, as
  specified by the \code{whiten_dims} parameter.
}
Additionally, if you set \code{ret_extra} to a vector of names, these will
be returned in addition to the values given above. These values are optional
and must be explicitly asked for, because they are either expensive to
calculate, take up a lot of memory, or both. The avaiable optional values
are:
\itemize{
\item{\code{X}} The input data, after filtering and scaling.
\item{\code{P}} The input probabilities.
\item{\code{Q}} The output probabilities.
\item{\code{DX}} Input distance matrix. The same as \code{X} when the input
  data is already a distance matrix.
\item{\code{DY}} Output coordinate distance matrix.
}
}
\description{
Embed a dataset using t-SNE.
}
\examples{
\dontrun{
colors = rainbow(length(unique(iris$Species)))
names(colors) = unique(iris$Species)
ecb = function(x, y) {
  plot(x, t = 'n')
  text(x, labels = iris$Species, col = colors[iris$Species])
}
# verbose = TRUE logs progress to console
tsne_iris <- tsne(iris, epoch_callback = ecb, perplexity = 50, verbose = TRUE)
# Use the early exaggeration suggested by Linderman and Steinerberger
tsne_iris_ls <- tsne(iris, epoch_callback = ecb, perplexity = 50,
                     exaggeration_factor = "ls")
# Make embedding deterministic by initializing with scaled PCA scores
tsne_iris_spca <- tsne(iris, epoch_callback = ecb, perplexity = 50,
                       exaggeration_factor = "ls", Y_init = "spca")
# Return extra details about the embedding
tsne_iris_extra <- tsne(iris, epoch_callback = ecb, perplexity = 50,
                        exaggeration_factor = "ls", Y_init = "spca", ret_extra = TRUE)

# Return even more details (which can be slow to calculate or take up a lot of memory)
tsne_iris_xextra <- tsne(iris, epoch_callback = ecb, perplexity = 50,
                        exaggeration_factor = "ls", Y_init = "spca",
                        ret_extra = c("P", "Q", "X", "DX", "DY"))
}
}
\references{
Van der Maaten, L., & Hinton, G. (2008).
Visualizing data using t-SNE.
\emph{Journal of Machine Learning Research}, \emph{9} (2579-2605).
\url{http://www.jmlr.org/papers/v9/vandermaaten08a.html}

Linderman, G. C., & Steinerberger, S. (2017).
Clustering with t-SNE, provably.
\emph{arXiv preprint} \emph{arXiv}:1706.02582.
\url{https://arxiv.org/abs/1706.02582}
}
