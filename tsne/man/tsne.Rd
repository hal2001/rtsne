% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tsne.R
\name{tsne}
\alias{tsne}
\title{Embed a dataset using t-distributed stochastic neighbor embedding.}
\usage{
tsne(X, k = 2, scale = "range", init = "rand", perplexity = 30,
  inp_kernel = "gauss", max_iter = 1000, whiten = FALSE,
  whiten_dims = 30, epoch_callback = NULL,
  epoch = base::round(max_iter/10), min_cost = 0, momentum = 0.5,
  final_momentum = 0.8, mom_switch_iter = 250, eta = 500,
  min_gain = 0.01, exaggeration_factor = 4, stop_lying_iter = 100,
  verbose = FALSE)
}
\arguments{
\item{X}{Input coordinates or distance matrix.}

\item{k}{Number of output dimensions for the embedding.}

\item{scale}{If \code{TRUE}, scale each column to zero mean and unit
variance. Alternatively, you may specify one of the following strings:
\code{"range"}, which range scales the matrix elements between 0 and 1;
\code{"bh"}, which applies the same scaling in Barnes-Hut t-SNE, where the
columns are mean centered and then the elements divided by absolute maximum
value; \code{"scale"} does the same as using \code{TRUE}. To use the input
data as-is, use \code{FALSE}, \code{NULL} or \code{"none"}.}

\item{init}{How to initialize the output coordinates. One of: \code{"rand"},
which initializes from a Gaussian distribution with mean 0 and standard
deviation 1e-4; \code{"pca"}, which uses the first \code{k} scores of the
PCA: columns are centered, but no scaling beyond that which is applied by
the \code{scale} parameter is carried out; \code{"spca"}, which uses the
PCA scores and then scales each score to a standard deviation of 1e-4; or a
matrix can be used to set the coordinates directly. It must have dimensions
\code{n} by \code{k}, where \code{n} is the number of rows in \code{X}.}

\item{perplexity}{The target perplexity for parameterizing the input
probabilities.}

\item{inp_kernel}{The input kernel function. Can be either \code{"gauss"}
(the default), or \code{"exp"}, which uses the unsquared distances.
\code{"exp"} is not the usual literature function, but matches the original
rtsne implementation (and it probably doesn't matter very much).}

\item{max_iter}{Maximum number of iterations in the optimization.}

\item{whiten}{If \code{TRUE}, whitens the input data before calculating the
input probabilities.}

\item{whiten_dims}{Number of dimensions to use if the data is preprocessed by
whitening. Must not be greater than the number of columns in \code{X}.}

\item{epoch_callback}{Function to call after each epoch. Should have the
signature \code{epoch_callback(Y)} where \code{Y} is the output coordinate
matrix.}

\item{epoch}{After every \code{epoch} number of steps, calculates and
displays the cost value and calls \code{epoch_callback}, if supplied.}

\item{min_cost}{If the cost falls below this value, the optimization will
stop early.}

\item{momentum}{Initial momentum value.}

\item{final_momentum}{Final momentum value.}

\item{mom_switch_iter}{Iteration at which the momentum will switch from
\code{momentum} to \code{final_momentum}.}

\item{eta}{Learning rate value.}

\item{min_gain}{Minimum gradient descent step size.}

\item{exaggeration_factor}{Numerical value to multiply input probabilities by, during
the early exaggeration phase. Not used if \code{initial_config} is not
\code{NULL}. May also provide the string \code{"ls"}, in which case the
dataset-dependent exaggeration technique suggested by Linderman and
Steinerberger (2017) is used.}

\item{stop_lying_iter}{Iteration at which early exaggeration is turned
off.}

\item{verbose}{If \code{TRUE}, log progress messages to the console.}
}
\value{
The embedded output coordinates.
}
\description{
Embed a dataset using t-distributed stochastic neighbor embedding.
}
\examples{
\dontrun{
colors = rainbow(length(unique(iris$Species)))
names(colors) = unique(iris$Species)
ecb = function(x, y) {
  plot(x, t = 'n')
  text(x, labels = iris$Species, col = colors[iris$Species])
}
# verbose = TRUE logs progress to console
tsne_iris <- tsne(iris[, -5], epoch_callback = ecb, perplexity = 50, verbose = TRUE)
# Use the early exaggeration suggested by Linderman and Steinerberger
tsne_iris_ls <- tsne(iris[, -5], epoch_callback = ecb, perplexity = 50,
                     exaggeration_factor = "ls")
# Make embedding deterministic by initializing with scaled PCA scores
tsne_iris_spca <- tsne(iris[, -5], epoch_callback = ecb, perplexity = 50,
                       exaggeration_factor = "ls", scale = "spca")
}
}
\references{
Van der Maaten, L., & Hinton, G. (2008).
Visualizing data using t-SNE.
\emph{Journal of Machine Learning Research}, \emph{9} (2579-2605).
\url{http://www.jmlr.org/papers/v9/vandermaaten08a.html}

Linderman, G. C., & Steinerberger, S. (2017).
Clustering with t-SNE, provably.
\emph{arXiv preprint} \emph{arXiv}:1706.02582.
\url{https://arxiv.org/abs/1706.02582}
}
