% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tsne.R
\name{tsne}
\alias{tsne}
\title{Embed a dataset using t-distributed stochastic neighbor embedding.}
\usage{
tsne(X, initial_config = NULL, k = 2, perplexity = 30, max_iter = 1000,
  whiten = FALSE, whiten_dims = 30, init_from_PCA = FALSE,
  epoch_callback = NULL, epoch = 100, min_cost = 0, momentum = 0.5,
  final_momentum = 0.8, mom_switch_iter = 250, eta = 500,
  min_gain = 0.01, exaggerate = 4, exaggeration_off_iter = 100)
}
\arguments{
\item{X}{Input coordinates or distance matrix.}

\item{initial_config}{Initial coordinates for the output coordinates.}

\item{k}{Number of output dimensions for the embeddding.}

\item{perplexity}{The target perplexity for parameterizing the input
probabilities.}

\item{max_iter}{Maximum number of iterations in the optimization.}

\item{whiten}{If \code{TRUE}, whitens the input data before calculating the
input probabilities.}

\item{whiten_dims}{Number of dimensions to use if the data is preprocessed
by whitening. Must not be greater than the number of columns in \code{X}.}

\item{init_from_PCA}{if set to \code{TRUE}, output coordinates are
initialized from the first \code{k} scores of the PCA. If not \code{TRUE},
then the output coordinates are initialized from a Gaussian distribution
with mean 0 and standard deviation 1e-4. Ignored if \code{initial_config} is
not \code{NULL}.}

\item{epoch_callback}{Function to call after each epoch. Should have the
signature \code{epoch_callback(Y)} where \code{Y} is the output
coordinate matrix.}

\item{epoch}{After every \code{epoch} number of steps, calculates and
displays the cost value and calls \code{epoch_callback}, if supplied.}

\item{min_cost}{If the cost falls below this value, the optimization will
stop early.}

\item{momentum}{Initial momentum value.}

\item{final_momentum}{Final momentum value.}

\item{mom_switch_iter}{Iteration at which the momentum will switch from
\code{momentum} to \code{final_momentum}.}

\item{eta}{Learning rate value.}

\item{min_gain}{Minimum gradient descent step size.}

\item{exaggerate}{Numerical value to multiply input probabilities by,
during the early exaggeration phase. Not used if \code{initial_config} is not
\code{NULL}. May also provide the string \code{"ls"}, in which case the
dataset-dependent exaggeration technique suggested by Linderman and
Steinerberger (2017) is used.}

\item{exaggeration_off_iter}{Iteration at which early exaggeration is turned
off.}
}
\value{
The embedded output coordinates.
}
\description{
Embed a dataset using t-distributed stochastic neighbor embedding.
}
\examples{
\dontrun{
colors = rainbow(length(unique(iris$Species)))
names(colors) = unique(iris$Species)
ecb = function(x, y) {
  plot(x, t = 'n')
  text(x, labels = iris$Species, col = colors[iris$Species])
}
tsne_iris <- tsne(iris[, -5], epoch_callback = ecb, perplexity = 50)
# Use the early exaggeration suggested by Linderman and Steinerberger
tsne_iris_ls <- tsne(iris[, -5], epoch_callback = ecb, perplexity = 50, exaggerate = "ls")
}
}
\references{
Van der Maaten, L., & Hinton, G. (2008).
Visualizing data using t-SNE.
\emph{Journal of Machine Learning Research}, \emph{9} (2579-2605).
\url{http://www.jmlr.org/papers/v9/vandermaaten08a.html}

Linderman, G. C., & Steinerberger, S. (2017).
Clustering with t-SNE, provably.
\emph{arXiv preprint} \emph{arXiv}:1706.02582.
\url{https://arxiv.org/abs/1706.02582}
}
