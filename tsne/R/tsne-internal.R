# Given a vector of distances and an exponential parameter beta, calculates
# the probabilities and corresponding Shannon entropy.
#
# Returns a list containing the Shannon entropy and the probability.
.Hbeta <- function(D, beta) {
  P <- exp(-D * beta)
  sumP <- sum(P)
  if (sumP == 0) {
    H <- 0
    P <- D * 0
  } else {
    H <- log(sumP) + beta * sum(D %*% P) / sumP
    P <- P / sumP
  }
  list(H = H, P = P)
}

# Calculates the input probabilities from X, such that each row probability
# distribution has the specified perplexity (within the supplied tolerance).
# Returns a list containing the probabilities and beta values.
# NB the default kernel, "exp", differs from the procedure in the TSNE paper by
# exponentially weighting the distances, rather than the squared distances.
# Set the kernel to "gauss" to get the squared distance version.
.x2p <- function(X, perplexity = 15, tol = 1e-5, kernel = "exp",
                 verbose = FALSE) {
  if (methods::is(X, "dist")) {
    D <- X
  } else {
    D <- stats::dist(X)
  }
  n <- attr(D, "Size")

  D <- as.matrix(D)
  if (kernel == "gauss") {
    D <- D * D
  }

  P <- matrix(0, n, n)
  beta <- rep(1, n)
  logU <- log(perplexity)
  perps <- rep(1, n)

  for (i in 1:n) {
    betamin <- -Inf
    betamax <- Inf
    Di <- D[i, -i]
    hbeta <- .Hbeta(Di, beta[i])
    H <- hbeta$H
    thisP <- hbeta$P
    Hdiff <- H - logU
    tries <- 0

    while (abs(Hdiff) > tol && tries < 50) {
      if (Hdiff > 0) {
        betamin <- beta[i]
        if (is.infinite(betamax)) {
          beta[i] <- beta[i] * 2
        } else {
          beta[i] <- (beta[i] + betamax) / 2
        }
      } else {
        betamax <- beta[i]
        if (is.infinite(betamin)) {
          beta[i] <- beta[i] / 2
        } else {
          beta[i] <- (beta[i] + betamin) / 2
        }
      }

      hbeta <- .Hbeta(Di, beta[i])
      H <- hbeta$H
      thisP <- hbeta$P
      Hdiff <- H - logU
      tries <- tries + 1
      # initialize guess for next point with optimized beta for this point
      # doesn't save many iterations, but why not?
      if (i < n) {
        beta[i + 1] <- beta[i]
      }
    }
    P[i, -i] <- thisP
    perps[i] <- exp(H)
  }
  sigma <- sqrt(1 / beta)

  if (verbose) {
    summary_sigma <- summary(sigma, digits = max(3, getOption("digits") - 3))
    message(date(), " sigma summary: ",
            paste(names(summary_sigma), ":", summary_sigma, "|", collapse = ""))
  }
  list(P = P, beta = beta)
}

# Whitens the input matrix X using n.comp components.
# Returns the whitened matrix.
.whiten <- function(X, row.norm = FALSE, verbose = FALSE, n.comp = ncol(X)) {
  n.comp  # forces an eval/save of n.comp
  if (verbose) {
    message("Centering")
  }
  n <- nrow(X)
  p <- ncol(X)
  X <- scale(X, scale = FALSE)
  if (row.norm) {
    X <- t(scale(X, scale = row.norm))
  } else {
    X <- t(X)
  }

  if (verbose) {
    message("Whitening")
  }
  V <- X %*% t(X) / n
  s <- La.svd(V)
  D <- diag(c(1 / sqrt(s$d)))
  K <- D %*% t(s$u)
  K <- matrix(K[1:n.comp, ], n.comp, p)
  X <- t(K %*% X)
  X
}

# Calculates a matrix containing the first ncol columns of the PCA scores.
# Returns the score matrix.
.scores_matrix <- function(X, ncol = min(nrow(X), base::ncol(X)),
                           verbose = FALSE) {
  X <- scale(X, center = TRUE, scale = FALSE)
  # do SVD on X directly rather than forming covariance matrix
  ncomp <- ncol
  s <- svd(X, nu = ncomp, nv = 0)
  D <- diag(c(s$d[1:ncomp]))
  if (verbose) {
    # calculate eigenvalues of covariance matrix from singular values
    lambda <- (s$d ^ 2) / (nrow(X) - 1)
    varex <- sum(lambda[1:ncomp]) / sum(lambda)
    message("PCA: ", ncomp, " components explained ", formatC(varex * 100),
            "% variance")
  }
  s$u %*% D
}
